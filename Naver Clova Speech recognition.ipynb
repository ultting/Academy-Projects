{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "840e5fdf",
   "metadata": {},
   "source": [
    "# Naver CLOVA Speech Api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "4256b9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 형태소 분류 함수\n",
    "def lemmatize(word):\n",
    "    morphtags = Komoran().pos(word)\n",
    "    if morphtags[0][1] == 'NNG' or morphtags[0][1] == 'NNP':\n",
    "        return morphtags[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3be5ec4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 라이브러리 및 모델 불러오기\n",
    "\n",
    "from flask import Flask ,render_template,request, redirect\n",
    "\n",
    "from konlpy.tag import Kkma,Okt, Twitter, Komoran # 형태소 분석 라이브러리\n",
    "import kss # 텍스트 문장으로 바꾸는 라이브러리\n",
    "from moviepy.editor import * # 영상을 오디오 파일로 변환\n",
    "from moviepy.editor import VideoFileClip, concatenate_videoclips\n",
    "import moviepy.editor as mp\n",
    "from pytube import YouTube # 유튜브 영상 다운로드 또는 불러오기\n",
    "import pytube\n",
    "import tqdm as tq\n",
    "\n",
    "\n",
    "import speech_recognition as sr # 오디오 파일 또는 음성을 텍스트로 변환\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# BOW = BAG of WORD : 단어가방, 단어모음, 단어사전\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# 위 도구는 빈도수 기반 벡터화 도구\n",
    "import requests\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d3acea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Korean Sentence Splitter]: C:\\Users\\smhrd\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app \"__main__\" (lazy loading)\n",
      " * Environment: production\n",
      "   WARNING: This is a development server. Do not use it in a production deployment.\n",
      "   Use a production WSGI server instead.\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Korean Sentence Splitter]:  * Running on http://61.80.106.115:3306/ (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ImmutableMultiDict([('link', 'https://www.youtube.com/watch?v=b3vQGkkklgo')])\n",
      "[라이브러리] 데이터분석에 관한 기초이론(Data Analysis)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Korean Sentence Splitter]: <ipython-input-83-1151f18cbf0d>:96: DeprecationWarning: Call to deprecated function all (This object can be treated as a list, all() is useless).\n",
      "  stream = yt.streams.all()[0]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'{\"language\": \"ko-KR\", \"completion\": \"sync\", \"callback\": null, \"userdata\": null, \"wordAlignment\": true, \"fullText\": true, \"forbiddens\": null, \"boostings\": null, \"diarization\": null}'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Korean Sentence Splitter]: C:\\Users\\smhrd\\anaconda3\\lib\\site-packages\\konlpy\\tag\\_okt.py:16: UserWarning: \"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.\n",
      "  warn('\"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.')\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['안녕하다'], ['오늘', '데이터', '분석', '기초', '이론', '함께', '살펴보다', '하다'], ['저', '스마트미디어', '인재', '개발', '원', '황해도', '연구원'], ['먼저', '저희', '데이터', '분석', '개념', '대해', '서', '한번', '살펴보다', '가다'], ['우리', '데이터', '분석', '거', '유용하다', '정보', '발굴', '하고', '결론', '내용', '알리', '며', '의사결정', '지원', '하다', '것', '목표', '로', '데이터', '정리', '변환', '모델링', '하다', '과정', '데이터', '분석', '말', '하다']]\n",
      "성공\n",
      "성공\n",
      "성공\n",
      "성공\n",
      "성공\n",
      "성공\n",
      "last [<moviepy.video.io.VideoFileClip.VideoFileClip object at 0x00000261322C03A0>, <moviepy.video.io.VideoFileClip.VideoFileClip object at 0x00000261322C0F10>, <moviepy.video.io.VideoFileClip.VideoFileClip object at 0x00000261322B35B0>, <moviepy.video.io.VideoFileClip.VideoFileClip object at 0x00000261323377F0>, <moviepy.video.io.VideoFileClip.VideoFileClip object at 0x00000261321ABFD0>, <moviepy.video.io.VideoFileClip.VideoFileClip object at 0x0000026132328D90>]\n",
      "Moviepy - Building video C:/Users/smhrd/git/BRIDGE_spring/Signal/src/main/webapp/WEB-INF/video/sua8.mp4.\n",
      "Moviepy - Writing video C:/Users/smhrd/git/BRIDGE_spring/Signal/src/main/webapp/WEB-INF/video/sua8.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Korean Sentence Splitter]: 61.80.106.115 - - [22/Jan/2022 16:37:05] \"\u001b[32mPOST /post HTTP/1.1\u001b[0m\" 302 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready C:/Users/smhrd/git/BRIDGE_spring/Signal/src/main/webapp/WEB-INF/video/sua8.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Korean Sentence Splitter]: 167.94.138.41 - - [22/Jan/2022 21:40:01] code 400, message Bad request version ('À\\x14À')\n",
      "[Korean Sentence Splitter]: 167.94.138.41 - - [22/Jan/2022 21:40:01] \"\u001b[35m\u001b[1m\u0016\u0003\u0001\u0000î\u0001\u0000\u0000ê\u0003\u0003\u001d",
      "®¿Ð!",
      ";;Q77\u0007]m\t\u0010ö¼v«´ú¶'ZÎ\u0015é )\u0004Yôúá\u0002Ä!*\u0002î95\u000erl\u0017\u0004\"¾\u000b",
      "Hõ\u0007\"Þ\u001c",
      "\u0000&Ì¨Ì©À/À0À+À,À\u0013À\tÀ\u0014À\u001b[0m\" HTTPStatus.BAD_REQUEST -\n",
      "[Korean Sentence Splitter]: 167.94.138.41 - - [22/Jan/2022 21:40:03] \"\u001b[33mGET / HTTP/1.1\u001b[0m\" 404 -\n",
      "[Korean Sentence Splitter]: 167.94.138.41 - - [22/Jan/2022 21:40:03] \"\u001b[33mGET / HTTP/1.1\u001b[0m\" 404 -\n"
     ]
    }
   ],
   "source": [
    "# 형태소 분류 함수\n",
    "def lemmatize(word):\n",
    "    morphtags = Komoran().pos(word)\n",
    "    if morphtags[0][1] == 'NNG' or morphtags[0][1] == 'NNP':\n",
    "        return morphtags[0][0]\n",
    "    \n",
    "    \n",
    "# naver CLOVA speech recognition API\n",
    "class ClovaSpeechClient:\n",
    "    # Clova Speech invoke URL\n",
    "    invoke_url = ''\n",
    "    # Clova Speech secret key\n",
    "    secret = ''\n",
    "\n",
    "    def req_url(self, url, completion, callback=None, userdata=None, forbiddens=None, boostings=None, wordAlignment=True, fullText=True, diarization=None):\n",
    "        request_body = {\n",
    "            'url': url,\n",
    "            'language': 'ko-KR',\n",
    "            'completion': completion,\n",
    "            'callback': callback,\n",
    "            'userdata': userdata,\n",
    "            'wordAlignment': wordAlignment,\n",
    "            'fullText': fullText,\n",
    "            'forbiddens': forbiddens,\n",
    "            'boostings': boostings,\n",
    "            'diarization': diarization,\n",
    "        }\n",
    "        headers = {\n",
    "            'Accept': 'application/json;UTF-8',\n",
    "            'Content-Type': 'application/json;UTF-8',\n",
    "            'X-CLOVASPEECH-API-KEY': self.secret\n",
    "        }\n",
    "        return requests.post(headers=headers,\n",
    "                             url=self.invoke_url + '/recognizer/url',\n",
    "                             data=json.dumps(request_body).encode('UTF-8'))\n",
    "\n",
    "    def req_object_storage(self, data_key, completion, callback=None, userdata=None, forbiddens=None, boostings=None,\n",
    "                           wordAlignment=True, fullText=True, diarization=None):\n",
    "        request_body = {\n",
    "            'dataKey': data_key,\n",
    "            'language': 'ko-KR',\n",
    "            'completion': completion,\n",
    "            'callback': callback,\n",
    "            'userdata': userdata,\n",
    "            'wordAlignment': wordAlignment,\n",
    "            'fullText': fullText,\n",
    "            'forbiddens': forbiddens,\n",
    "            'boostings': boostings,\n",
    "            'diarization': diarization,\n",
    "        }\n",
    "        headers = {\n",
    "            'Accept': 'application/json;UTF-8',\n",
    "            'Content-Type': 'application/json;UTF-8',\n",
    "            'X-CLOVASPEECH-API-KEY': self.secret\n",
    "        }\n",
    "        return requests.post(headers=headers,\n",
    "                             url=self.invoke_url + '/recognizer/object-storage',\n",
    "                             data=json.dumps(request_body).encode('UTF-8'))\n",
    "\n",
    "    def req_upload(self, file, completion, callback=None, userdata=None, forbiddens=None, boostings=None,\n",
    "                   wordAlignment=True, fullText=True, diarization=None):\n",
    "        request_body = {\n",
    "            'language': 'ko-KR',\n",
    "            'completion': completion,\n",
    "            'callback': callback,\n",
    "            'userdata': userdata,\n",
    "            'wordAlignment': wordAlignment,\n",
    "            'fullText': fullText,\n",
    "            'forbiddens': forbiddens,\n",
    "            'boostings': boostings,\n",
    "            'diarization': diarization,\n",
    "        }\n",
    "        headers = {\n",
    "            'Accept': 'application/json;UTF-8',\n",
    "            'X-CLOVASPEECH-API-KEY': self.secret\n",
    "        }\n",
    "        print(json.dumps(request_body, ensure_ascii=False).encode('UTF-8'))\n",
    "        files = {\n",
    "            'media': open(file, 'rb'),\n",
    "            'params': (None, json.dumps(request_body, ensure_ascii=False).encode('UTF-8'), 'application/json')\n",
    "        }\n",
    "        response = requests.post(headers=headers, url=self.invoke_url + '/recognizer/upload', files=files)\n",
    "        return response\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/post',methods=['POST','GET'])\n",
    "def result():\n",
    "    if request.method == 'POST' :\n",
    "        result = request.form\n",
    "        print(result)\n",
    "        yt = pytube.YouTube(result.get('link'))\n",
    "        print(yt.title)\n",
    "        \n",
    "        # 유튜브 영상 다운로드 후 저장\n",
    "        stream = yt.streams.all()[0]\n",
    "        stream.download(output_path='test/data')\n",
    "        \n",
    "        # 영상 제목\n",
    "        title = yt.title\n",
    "        \n",
    "        # CLOVA Api 는 req관련 코드가 2개가 더 있음\n",
    "        res = ClovaSpeechClient().req_upload(file='test/data/'+title+'.3gpp', completion='sync')\n",
    "        #print(res.text)\n",
    "        \n",
    "        # 전체 텍스트를 json 타입 변수에 저장\n",
    "        # 텍스트 추출\n",
    "        text = json.loads(res.text)\n",
    "\n",
    "        # kss 활용 텍스트 문장 화\n",
    "        word_list = kss.split_sentences(text['text'])\n",
    "        \n",
    "        # 문장 끝 마침표 제거\n",
    "        for i in range(len(word_list)):\n",
    "            word_list[i]=word_list[i].replace('.','')\n",
    "            \n",
    "            \n",
    "            # 명사만 가져오기 위한 삭제\n",
    "        okt = Okt()\n",
    "        headline = []\n",
    "        stopwords = [ '의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','등','으로도']\n",
    "        for sentence in word_list:\n",
    "            temp = []\n",
    "            # morphs() : 형태소 단위로 토큰화\n",
    "            # stem = True : 형태소에서 어간을 추출\n",
    "            temp = okt.morphs(sentence, stem = True)\n",
    "            temp = [word for word in temp if not word in stopwords]\n",
    "            headline.append(temp)\n",
    "\n",
    "           # konlpy 트위터 이용 형태소 분류\n",
    "        twitter = Twitter()\n",
    "        sentences_tag_last=[]\n",
    "        for word in headline:\n",
    "            sentences_tag=[]\n",
    "            for i in word :\n",
    "                morph = twitter.pos(i)\n",
    "                sentences_tag.append(morph)\n",
    "            sentences_tag_last.append(sentences_tag)\n",
    "\n",
    "        #  형태소 분류\n",
    "        adj_list_last=[]\n",
    "        for ko in sentences_tag_last:\n",
    "            noun_adj_list=[]\n",
    "            for i in ko:\n",
    "                for word, tag in i:\n",
    "                    if tag in ['Noun','Verb','Number','Adjective','Adverb','Alpha']:\n",
    "                        noun_adj_list.append(word)\n",
    "            adj_list_last.append(noun_adj_list)\n",
    "\n",
    "        print(adj_list_last)\n",
    "        # 형태소 분류 Komoran\n",
    "        for i in range(len(adj_list_last)):\n",
    "            for j in range(len(adj_list_last[i])):\n",
    "                if lemmatize(adj_list_last[i][j]) != None :\n",
    "                        adj_list_last[i][j] = lemmatize(adj_list_last[i][j])\n",
    "\n",
    "        arr_list = adj_list_last\n",
    "\n",
    "        wordCount=[]\n",
    "        for i in range(len(arr_list)):\n",
    "            for j in range(len(arr_list[i])):\n",
    "                wordCount.append(arr_list[i][j])\n",
    "        # 영상 합치기\n",
    "        # 데이터프레임 \n",
    "        wordData=pd.read_csv('Data_Deep/word_data.csv')\n",
    "\n",
    "        # 데이터프레임에 있는 json 과 단어를 뽑아서 2차원 리스트로 만들기\n",
    "        wordList = []\n",
    "        for i in range(len(wordData)):\n",
    "            jsonList=[]\n",
    "            for j in range(1):\n",
    "                jsonList.append(wordData.iloc[i,1])\n",
    "                jsonList.append(wordData.iloc[i,2])\n",
    "                jsontuple = tuple(jsonList)\n",
    "            wordList.append(jsontuple)\n",
    "\n",
    "        # 2차원 리스트 ( wordList )안에 샘플데이터 ( testList ) 가 있는지 확인\n",
    "        jsonList2 = []\n",
    "        for i in range(len(wordList)):\n",
    "            if wordList[i][0] in wordCount:\n",
    "                jsonList2.append(wordList[i][1]) # 맞는 번호의 json파일 \n",
    "\n",
    "        jsonFileName=[]\n",
    "        json_data=[]\n",
    "        for i in range(len(jsonList2)):\n",
    "            # 3. json파일 오픈\n",
    "            jsonMovieData=[]\n",
    "            for j in range(1):\n",
    "                with open('Data_Deep/3000/'+jsonList2[i],'r',encoding='utf-8') as f:\n",
    "                    json_data.append(json.load(f))\n",
    "                    #print(json.dumps(json_data))\n",
    "                    jsonMovieData.append(json_data[i]['metaData']['name'])\n",
    "                    jsonMovieData.append(json_data[i]['data'][0]['start'])\n",
    "                    jsonMovieData.append(json_data[i]['data'][0]['end'])\n",
    "            jsonFileName.append(jsonMovieData)\n",
    "\n",
    "\n",
    "        clips = []\n",
    "        try:\n",
    "            for i in range(len(jsonFileName)):\n",
    "                mov = VideoFileClip('Data_Deep/Wordmp4/real_word_3000/'+jsonFileName[i][0]).subclip(jsonFileName[i][1],jsonFileName[i][2])\n",
    "                mov = mov.resize(height=1080,width=1920) # 크기 맞추기\n",
    "                clips.append(mov)\n",
    "                print('성공')\n",
    "        except:\n",
    "            print('skip')\n",
    "        print('last',clips)\n",
    "        path = 'sua8.mp4'\n",
    "        final_clip = concatenate_videoclips(clips, method='compose')\n",
    "        final_clip.write_videofile('C:/Users/smhrd/git/BRIDGE_spring/Signal/src/main/webapp/WEB-INF/video/'+path)\n",
    "\n",
    "        \n",
    "        return redirect(\"http://localhost:8082/web/detailpage?link=\"+path)\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    app.run(host= '61.80.106.115', port=3306)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40134da0",
   "metadata": {},
   "source": [
    "# Google Cloud Speech To Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7133f091",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# 유튜브 영상 다운로드 후 저장\n",
    "\n",
    "stream = yt.streams.all()[0]\n",
    "stream.download(output_path='C:/Users/smhrd/Desktop/Machine Learning/test/data') \n",
    "\n",
    "# 영상을 오디오 파일로 변환 \n",
    "clip = mp.VideoFileClip(\"data/[파이썬 기초] NO3 변수.3gpp\")\n",
    "newsound = clip.subclip(\"00:00:10\",\"00:01:00\") # 20 sec\n",
    "newsound.audio.write_audiofile(\"data/[파이썬 기초] NO3 변수.wav\",16000,2,2000,'pcm_s16le')\n",
    "\n",
    "# 오디오 파일 로드\n",
    "filename = \"data/[파이썬 기초] NO3 변수.wav\"\n",
    "\n",
    "# 오디오 파일 텍스트 추출\n",
    "text = []\n",
    "r = sr.Recognizer()\n",
    "with sr.AudioFile(filename) as source:\n",
    "    audio_data = r.record(source)\n",
    "    text = r.recognize_google(audio_data,language='ko-KR')\n",
    "    # print(text)\n",
    "\n",
    "# kss 활용 텍스트 문장 화\n",
    "word_list = kss.split_sentences(text)\n",
    "\n",
    "# 명사만 가져오기 위한 삭제\n",
    "okt = Okt()\n",
    "headline = []\n",
    "stopwords = [ '의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','등','으로도']\n",
    "for sentence in word_list:\n",
    "    temp = []\n",
    "    # morphs() : 형태소 단위로 토큰화\n",
    "    # stem = True : 형태소에서 어간을 추출\n",
    "    temp = okt.morphs(sentence, stem = True)\n",
    "    temp = [word for word in temp if not word in stopwords]\n",
    "    headline.append(temp)\n",
    "    \n",
    "\n",
    "# konlpy 트위터 이용 형태소 분류\n",
    "twitter = Twitter()\n",
    "sentences_tag = []\n",
    "for word in headline:\n",
    "    for i in word :\n",
    "        morph = twitter.pos(i)\n",
    "        sentences_tag.append(morph)\n",
    "# print(sentences_tag)\n",
    "\n",
    "# -\n",
    "\n",
    "#  형태소 분류\n",
    "noun_adj_list=[]\n",
    "for i1 in sentences_tag:\n",
    "    for word, tag in i1:\n",
    "        if tag in ['Noun','Verb','Number','Adjective','Adverb','Alpha']:\n",
    "            noun_adj_list.append(word)\n",
    "# print(noun_adj_list)\n",
    "\n",
    "# 형태소 분류\n",
    "for i in range(len(noun_adj_list)):\n",
    "    #print(lemmatize(noun_adj_list[i]))\n",
    "    if lemmatize(noun_adj_list[i]) != None :\n",
    "        noun_adj_list[i] = lemmatize(noun_adj_list[i])\n",
    "        #print(noun_adj_list)\n",
    "\n",
    "arr_list = noun_adj_list\n",
    "print(arr_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137225e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 구글 정확도\n",
    "from google.cloud import speech_v1p1beta1 as speech\n",
    "\n",
    "client = speech.SpeechClient()\n",
    "\n",
    "speech_file = \"data/hello.wav\"\n",
    "\n",
    "with open(speech_file, \"rb\") as audio_file:\n",
    "    content = audio_file.read()\n",
    "\n",
    "audio = speech.RecognitionAudio(content=content)\n",
    "\n",
    "# 현재 코드는 default값을 기준으로 실행시 에러여서 카운트 2가 부여되어 있음\n",
    "# audio channel 관련 에러 시 채널 카운트를 2로 줄 것\n",
    "config = speech.RecognitionConfig( \n",
    "    encoding=speech.RecognitionConfig.AudioEncoding.LINEAR16,\n",
    "    sample_rate_hertz=16000,\n",
    "    language_code=\"ko-KR\",\n",
    "    enable_word_confidence=True,\n",
    "    audio_channel_count = 2\n",
    ")\n",
    "\n",
    "\n",
    "response = client.recognize(config=config, audio=audio)\n",
    "\n",
    "for i, result in enumerate(response.results):\n",
    "    alternative = result.alternatives[0]\n",
    "    print(\"-\" * 20)\n",
    "    print(\"First alternative of result {}\".format(i))\n",
    "    print(u\"Transcript: {}\".format(alternative.transcript))\n",
    "    print(\n",
    "        u\"First Word and Confidence: ({}, {})\".format(\n",
    "            alternative.words[0].word, alternative.words[0].confidence\n",
    "        )\n",
    "    )\n",
    "    \n",
    "## First Word and Confidence: (먼저, 0.8410878777503967)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d5dbbf",
   "metadata": {},
   "source": [
    "# 수어 어순 알고리즘 (실패)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e587ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_new_word_list = [] # 수어 문장으로 정렬 하는 리스트\n",
    "Noun_list=[] # 순서에 맞지 않았을때 값이 들어간 경우 \n",
    "Verb_list = [] # \n",
    "del_list=[] # 활용가능성이 없는 단어들\n",
    "for i in range(len(word_list_last)):\n",
    "    for j in range(len(word_list_last[i])):\n",
    "        if word_list_last[i][j][0][1]=='Noun':\n",
    "            if len(word_list_last[i][j][0][0])==1:\n",
    "                word_list_last[i].pop(j)\n",
    "        elif word_list_last[i][j][0][1]=='Verb':\n",
    "            if len(word_list_last[i][j][0][0])==1:\n",
    "                word_list_last[i].pop(j)\n",
    "\n",
    "for j in range(len(word_list_last)):\n",
    "    new_word_list = [] # 수어 문장으로 정렬 하는 리스트\n",
    "    for i in range(len(word_list_last[j])):\n",
    "        if word_list_last[j][i][0][1]=='Noun':\n",
    "            print(i)\n",
    "            Noun_list.append(word_list_last[j][i][0][0])\n",
    "            if len(new_word_list) < 3:  # 0,1,2\n",
    "                if word_list_last[j][i][0][1]=='Noun':\n",
    "                    new_word_list.insert(i,word_list_last[j][i][0][0])\n",
    "                elif word_list_last[j][i][0][1]=='Verb':\n",
    "                    Verb_list.append(word_list_last[j][i][0][1])\n",
    "            elif len(new_word_list) ==3:\n",
    "                if word_list_last[j][i][0][1]=='Verb':\n",
    "                    new_word_list.insert(i,word_list_last[j][i][0][0])\n",
    "                continue\n",
    "\n",
    "            if len(new_word_list) > 3 and len(new_word_list) < 7:\n",
    "                if word_list_last[j][i][0][1] =='Noun':\n",
    "                    new_word_list.insert(i,word_list_last[j][i][0][0])\n",
    "            elif len(new_word_list) ==7:\n",
    "                if word_list_last[j][i][0][1]=='Verb':\n",
    "                    new_word_list.insert(i,word_list_last[j][i][0][0])\n",
    "                continue\n",
    "\n",
    "            if len(new_word_list) > 7 and len(new_word_list) < 11:\n",
    "                if word_list_last[j][i][0][1] =='Noun':\n",
    "                     new_word_list.insert(i,word_list_last[j][i][0][0])\n",
    "                elif word_list_last[j][i][0][1] =='Verb':\n",
    "                     Verb_list.append(word_list_last[j][i][0][0])\n",
    "\n",
    "            elif len(new_word_list) ==11:\n",
    "                 if word_list_last[j][i][0][1]=='Verb':\n",
    "                        new_word_list.insert(i,word_list_last[j][i][0][0])\n",
    "                        continue\n",
    "\n",
    "            if len(new_word_list) > 11 and len(new_word_list) < 15:\n",
    "                if word_list_last[j][i][0][1] =='Noun':\n",
    "                    new_word_list.insert(i,word_list_last[j][i][0][0])\n",
    "                elif word_list_last[j][i][0][1] =='Verb':\n",
    "                    Verb_list.append(word_list_last[j][i][0][0])\n",
    "\n",
    "            elif len(new_word_list) ==15:\n",
    "                if word_list_last[j][i][0][1]=='Verb':\n",
    "                    new_word_list.insert(i,word_list_last[j][i][0][0])\n",
    "                continue\n",
    "\n",
    "            if len(new_word_list) > 15 and len(new_word_list) < 19:\n",
    "                if word_list_last[j][i][0][1] =='Noun':\n",
    "                    new_word_list.insert(i,word_list_last[j][i][0][0])\n",
    "                elif word_list_last[j][i][0][1] =='Verb':\n",
    "                    Verb_list.append(word_list_last[j][i][0][0])\n",
    "\n",
    "            elif len(new_word_list) ==19:\n",
    "                if word_list_last[j][i][0][1]=='Verb':\n",
    "                    new_word_list.insert(i,word_list_last[j][i][0][0])\n",
    "                continue        \n",
    "\n",
    "        if word_list_last[j][i][0][1] == 'Verb':\n",
    "            print(i)\n",
    "            if len(new_word_list) < 3:  # 0,1,2\n",
    "                if word_list_last[j][i][0][1]=='Noun':\n",
    "                    new_word_list.insert(i,word_list_last[j][i][0][0])\n",
    "                elif word_list_last[j][i][0][1]=='Verb':\n",
    "                    Verb_list.append(word_list_last[j][i][0][0])\n",
    "            elif len(new_word_list) ==3:\n",
    "                if word_list_last[j][i][0][1]=='Noun':\n",
    "                    if i == 1:\n",
    "                        del_list.append(word_list_last[j][i][0][0])\n",
    "                    else:\n",
    "                        print('Verb')\n",
    "                        Noun_list.append(word_list_last[j][i][0][0])\n",
    "                if word_list_last[j][i][0][1]=='Verb':\n",
    "                    new_word_list.insert(i,word_list_last[j][i][0][0])\n",
    "                continue\n",
    "\n",
    "            if len(new_word_list) > 3 and len(new_word_list) < 7:\n",
    "                if word_list_last[j][i][0][1] =='Noun':\n",
    "                    new_word_list.insert(i,word_list_last[j][i][0][0])\n",
    "            elif len(new_word_list) ==7:\n",
    "                if word_list_last[j][i][0][1]=='Verb':\n",
    "                    print(i)\n",
    "                    new_word_list.insert(i,word_list_last[j][i][0][0])\n",
    "                continue\n",
    "\n",
    "            if len(new_word_list) > 7 and len(new_word_list) < 11:\n",
    "                if word_list_last[j][i][0][1] =='Noun':   \n",
    "                    new_word_list.insert(word_list_last[j][i][0][0])\n",
    "                elif word_list_last[j][i][0][1] =='Verb':\n",
    "                    Verb_list.append(word_list_last[j][i][0][0])\n",
    "            elif len(new_word_list) ==11:\n",
    "                if word_list_last[j][i][0][1]=='Verb':\n",
    "                    new_word_list.insert(i,word_list_last[j][i][0][0])\n",
    "                continue\n",
    "\n",
    "            if len(new_word_list) > 11 and len(new_word_list) < 15:\n",
    "                if word_list_last[j][i][0][1] =='Noun':\n",
    "                    new_word_list.insert(i,word_list_last[j][i][0][0])\n",
    "                elif word_list_last[j][i][0][1] =='Verb':\n",
    "                    Verb_list.append(word_list_last[j][i][0][0])\n",
    "\n",
    "            elif len(new_word_list) ==15:\n",
    "                if word_list_last[j][i][0][1]=='Verb':\n",
    "                    new_word_list.insert(i,word_list_last[j][i][0][0])\n",
    "                continue     \n",
    "\n",
    "            if len(new_word_list) > 15 and len(new_word_list) < 19:\n",
    "                if word_list_last[j][i][0][1] =='Noun':\n",
    "                    new_word_list.insert(i,word_list_last[j][i][0][0])\n",
    "                elif word_list_last[j][i][0][1] =='Verb':\n",
    "                    Verb_list.append(word_list_last[j][i][0][0])\n",
    "\n",
    "            elif len(new_word_list) ==19:\n",
    "                if word_list_last[j][i][0][1]=='Verb':\n",
    "                    new_word_list.insert(i,word_list_last[j][i][0][0])\n",
    "                continue\n",
    "    \n",
    "print(new_word_list)\n",
    "print(Noun_list)\n",
    "print(del_list)\n",
    "print(Verb_list)\n",
    "print('Noun / Noun / Verb / Noun / Noun / Verb / Noun / Noun / Verb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ecce5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 분류 및 정렬\n",
    "\n",
    "SetNoun_list = set(Noun_list)\n",
    "SetNew_word_list = set(new_word_list)\n",
    "\n",
    "print(SetNoun_list.difference(SetNew_word_list))\n",
    "print(new_word_list)\n",
    "print(Noun_list)\n",
    "print(Verb_list)\n",
    "\n",
    "\n",
    "print(word_list_last[2])\n",
    "print(new_word_list)\n",
    "\n",
    "print(Noun_list)\n",
    "print(Verb_list)\n",
    "print('우리가 저번 시간까지 해서 매트릭스에 대해서 한번 배워봤는데 오늘 해볼 거는 데이터 프레임에 대해서 한번 배워보도록 하겠습니다')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37f64d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(new_word_list) > 14 and len(new_word_list) < 17:\n",
    "        if word_list_last[2][i][0][1] =='Noun':\n",
    "            new_word_list.insert(i,word_list_last[2][i][0][0])\n",
    "        elif word_list_last[2][i][0][1] =='Verb':\n",
    "            Verb_list.append(word_list_last[2][i][0][0])\n",
    "            \n",
    "    if len(new_word_list) ==17:\n",
    "        if word_list_last[2][i][0][1]=='Verb':\n",
    "            new_word_list.insert(i,word_list_last[2][i][0][0])            \n",
    "            \n",
    "    if len(new_word_list) > 17 and len(new_word_list) < 20:\n",
    "        if word_list_last[2][i][0][1] =='Noun':\n",
    "            new_word_list.insert(i,word_list_last[2][i][0][0])\n",
    "        elif word_list_last[2][i][0][1] =='Verb':\n",
    "            Verb_list.append(word_list_last[2][i][0][0])\n",
    "            \n",
    "    if len(new_word_list) ==20:\n",
    "        if word_list_last[2][i][0][1]=='Verb':\n",
    "            new_word_list.insert(i,word_list_last[2][i][0][0])\n",
    "            \n",
    "    if len(new_word_list) > 20 and len(new_word_list) < 23:\n",
    "        if word_list_last[2][i][0][1] =='Noun':\n",
    "            new_word_list.insert(i,word_list_last[2][i][0][0])\n",
    "        elif word_list_last[2][i][0][1] =='Verb':\n",
    "            Verb_list.append(word_list_last[2][i][0][0])\n",
    "            \n",
    "    if len(new_word_list) ==23:\n",
    "        if word_list_last[2][i][0][1]=='Verb':\n",
    "            new_word_list.insert(i,word_list_last[2][i][0][0])\n",
    "            \n",
    "    if len(new_word_list) > 23 and len(new_word_list) < 26:\n",
    "        if word_list_last[2][i][0][1] =='Noun':\n",
    "            new_word_list.insert(i,word_list_last[2][i][0][0])\n",
    "        elif word_list_last[2][i][0][1] =='Verb':\n",
    "            Verb_list.append(word_list_last[2][i][0][0])\n",
    "            \n",
    "    if len(new_word_list) ==26:\n",
    "        if word_list_last[2][i][0][1]=='Verb':\n",
    "            new_word_list.insert(i,word_list_last[2][i][0][0])            \n",
    "    \n",
    "    if len(new_word_list) > 26 and len(new_word_list) < 29:\n",
    "        if word_list_last[2][i][0][1] =='Noun':\n",
    "            new_word_list.insert(i,word_list_last[2][i][0][0])\n",
    "        elif word_list_last[2][i][0][1] =='Verb':\n",
    "            Verb_list.append(word_list_last[2][i][0][0])\n",
    "            \n",
    "    if len(new_word_list) ==29:\n",
    "        if word_list_last[2][i][0][1]=='Verb':\n",
    "            new_word_list.insert(i,word_list_last[2][i][0][0])            \n",
    "            \n",
    "    if len(new_word_list) > 29 and len(new_word_list) < 32:\n",
    "        if word_list_last[2][i][0][1] =='Noun':\n",
    "            new_word_list.insert(i,word_list_last[2][i][0][0])\n",
    "        elif word_list_last[2][i][0][1] =='Verb':\n",
    "            Verb_list.append(word_list_last[2][i][0][0])\n",
    "            \n",
    "    if len(new_word_list) ==32:\n",
    "        if word_list_last[2][i][0][1]=='Verb':\n",
    "            new_word_list.insert(i,word_list_last[2][i][0][0])            \n",
    "            \n",
    "    if len(new_word_list) > 32 and len(new_word_list) < 35:\n",
    "        if word_list_last[2][i][0][1] =='Noun':\n",
    "            new_word_list.insert(i,word_list_last[2][i][0][0])\n",
    "        elif word_list_last[2][i][0][1] =='Verb':\n",
    "            Verb_list.append(word_list_last[2][i][0][0])\n",
    "            \n",
    "    if len(new_word_list) ==35:\n",
    "        if word_list_last[2][i][0][1]=='Verb':\n",
    "            new_word_list.insert(i,word_list_last[2][i][0][0])            \n",
    "            \n",
    "    if len(new_word_list) > 35 and len(new_word_list) < 38:\n",
    "        if word_list_last[2][i][0][1] =='Noun':\n",
    "            new_word_list.insert(i,word_list_last[2][i][0][0])\n",
    "        elif word_list_last[2][i][0][1] =='Verb':\n",
    "            Verb_list.append(word_list_last[2][i][0][0])\n",
    "            \n",
    "    if len(new_word_list) ==38:\n",
    "        if word_list_last[2][i][0][1]=='Verb':\n",
    "            new_word_list.insert(i,word_list_last[2][i][0][0])            \n",
    "            \n",
    "    if len(new_word_list) > 38 and len(new_word_list) < 41:\n",
    "        if word_list_last[2][i][0][1] =='Noun':\n",
    "            new_word_list.insert(i,word_list_last[2][i][0][0])\n",
    "        elif word_list_last[2][i][0][1] =='Verb':\n",
    "            Verb_list.append(word_list_last[2][i][0][0])\n",
    "            \n",
    "    if len(new_word_list) ==41:\n",
    "        if word_list_last[2][i][0][1]=='Verb':\n",
    "            new_word_list.insert(i,word_list_last[2][i][0][0])            \n",
    "            \n",
    "    if len(new_word_list) > 41 and len(new_word_list) < 44:\n",
    "        if word_list_last[2][i][0][1] =='Noun':\n",
    "            new_word_list.insert(i,word_list_last[2][i][0][0])\n",
    "        elif word_list_last[2][i][0][1] =='Verb':\n",
    "            Verb_list.append(word_list_last[2][i][0][0])\n",
    "            \n",
    "    if len(new_word_list) ==44:\n",
    "        if word_list_last[2][i][0][1]=='Verb':\n",
    "            new_word_list.insert(i,word_list_last[2][i][0][0])            \n",
    "            \n",
    "    if len(new_word_list) > 44 and len(new_word_list) < 47:\n",
    "        if word_list_last[2][i][0][1] =='Noun':\n",
    "            new_word_list.insert(i,word_list_last[2][i][0][0])\n",
    "        elif word_list_last[2][i][0][1] =='Verb':\n",
    "            Verb_list.append(word_list_last[2][i][0][0])\n",
    "            \n",
    "    if len(new_word_list) ==47:\n",
    "        if word_list_last[2][i][0][1]=='Verb':\n",
    "            new_word_list.insert(i,word_list_last[2][i][0][0])            \n",
    "            \n",
    "    if len(new_word_list) > 47 and len(new_word_list) < 50:\n",
    "        if word_list_last[2][i][0][1] =='Noun':\n",
    "            new_word_list.insert(i,word_list_last[2][i][0][0])\n",
    "        elif word_list_last[2][i][0][1] =='Verb':\n",
    "            Verb_list.append(word_list_last[2][i][0][0])\n",
    "            \n",
    "    if len(new_word_list) ==50:\n",
    "        if word_list_last[2][i][0][1]=='Verb':\n",
    "            new_word_list.insert(i,word_list_last[2][i][0][0])            "
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "encoding": "# -*- coding: utf-8 -*-",
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
